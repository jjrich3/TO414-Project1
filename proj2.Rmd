---
title: "project2"
author: "Josh Richman"
date: "2023-11-02"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(neuralnet)
library(kernlab)
library(C50)
```


# Preparing Data
## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
ins <- read.csv("TravelInsurancePrediction.csv", stringsAsFactors = TRUE)
#summary(ins)
ins$X = NULL

str(ins)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

insmm <- as.data.frame(model.matrix(~.-1,ins))
str(insmm)

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything for KNN and ANN
ins_norm <- as.data.frame(lapply(insmm, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(ins_norm), 0.2*nrow(ins_norm)) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
ins_train <- ins_norm[-test_set, -match("TravelInsurance",names(ins_norm))]
ins_test <- ins_norm[test_set, -match("TravelInsurance",names(ins_norm))]

#Now the response (aka Labels) - only the yyes column
ins_train_labels <- ins_norm[-test_set, "TravelInsurance"]
ins_test_labels <- ins_norm[test_set, "TravelInsurance"]

#train and test set that includes yyes
ins_train_all = ins_norm[-test_set,]
ins_test_all = ins_norm[test_set,]
```

# Majority Vote Algorithms
## Functions
```{r}
#fp = false positives (customers we called but did not buy a term deposit).
#tp = true positives (customers we called and they did buy a term deposit).
#old_rate = old call success rate, based on test data from confusion matrix, usually around 0.11.
calcu = function(fp,tp, old_rate) {
  #revenue
  rev = tp * 11
  
  #cost
  total_calls = tp + fp
  new_rate = tp / total_calls
  avg_employee_calls = ((new_rate - old_rate) * 100 * 100) + 1000
  num_employees = ceiling(total_calls / avg_employee_calls)
  train_cost = num_employees * 1000
  cost = (total_calls * 1) + train_cost
  return(rev - cost)
}

#Finds best value to split data
find_max = function(pred){
  #Sequence of values for weighing predictions
  vals = seq(0.005,0.5,by=0.005)
  max = 0
  max_x = 0.005
  for (x in vals){
  preds = ifelse(pred > x, 1,0)
  cm = table(as.factor(preds), as.factor(ins_test_labels))
  fp = cm[2,1]
  tp = cm[2,2]
  old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
  hold = calcu(fp,tp, old_rate)
    if(hold > max){ 
      max = hold
      max_x = x
    }
  }
  return(c(max_x,max))
}
```

## logistical Regression
```{r}
library(caret)
logmod = glm(TravelInsurance ~  . + (. * .), data=ins_train_all, family = 'binomial')
#summary(logmod)
#stepfin = step(logmod, direction = "backward")

logpred = predict(logmod, ins_test_all)

maxs = find_max(logpred)

logpreds = ifelse(logpred > maxs[1], 1,0)
cm = table(as.factor(logpreds), as.factor(ins_test_labels))
cm

cm2 = confusionMatrix(as.factor(logpreds), as.factor(ins_test_labels),positive = "1")
cm2
```

## K-Nearest Neighbors
```{r}
library(caret)
set.seed(12345)

#may need to change k still
knnmod = knnreg(ins_train_labels ~ ., data=ins_train, k=7)
knnpred = predict(knnmod, ins_test)
#best appears to be k=30 at 0.125
#maxs = find_max(knnpred)
#maxs
knnpreds = ifelse(knnpred > 0.45 ,1,0)
cm = table(as.factor(knnpreds), as.factor(ins_test_labels))
#cm = confusionMatrix(as.factor(knnpreds),as.factor(tele_test_labels), positive = "1")
cm
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
knnprof = calcu(fp,tp, old_rate)
knnprof

```

## ANN
```{r}
# simple ANN with only a single hidden neuron
ANN_model <- neuralnet(formula = TravelInsurance ~ ., data = ins_train_all)


# visualize the network topology
plot(ANN_model)

## Step 4: Evaluating model performance ----
# obtain model results
model_results <- compute(ANN_model, ins_test_all[1:10])
# obtain predicted strength values
ann_predicted_yes <- model_results$net.result
annpreds = ifelse(ann_predicted_yes > 0.45 ,1,0)
cm = table(as.factor(annpreds), as.factor(ins_test_labels))
cm
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
annprof = calcu(fp,tp, old_rate)
annprof
```

## 4. SVM
```{r}
svm_mod = ksvm(TravelInsurance ~ . ,data = ins_train_all,kernel = 'rbfdot')
svmpred = predict(svm_mod, ins_test_all)

maxs = find_max(svmpred)
maxs
svmpreds = ifelse(svmpred > 0.45 ,1,0)
cm = table(as.factor(svmpreds), as.factor(ins_test_labels))
cm
#svmprof = calcu(cm)
#svmprof
```

## 5. Tree
```{r}
treemod = C5.0(as.factor(TravelInsurance) ~ ., data=ins_train_all)
treepred = predict(treemod, ins_test_all)
#confusionMatrix(as.factor(hrpred), as.factor(hr_test$left), positive="1")
cm = table(as.factor(treepred), as.factor(ins_test_labels))
cm
#treeprof = calcu(cm)
#treeprof
```

## Majority Vote
```{r}
#final predictions
ins_preds = data.frame(
  log = logpreds,
  knn = knnpreds,
  ann = annpreds,
  svm = svmpreds,
  tree = treepred,
  true = ins_test_labels
)
```

```{r}
#final second model can be better than all
#try to get higher kappa
set.seed(12345)

tree_train_idx = sample(1:nrow(ins_preds), 0.6*nrow(ins_preds))

tree_test = ins_preds[-tree_train_idx,]
tree_train = ins_preds[tree_train_idx,]

# find better cost
#error_cost = matrix(c(0,1,10,0), nrow=2)
#error_cost
tree2_mod = C5.0(as.factor(true) ~ ., data = ins_preds)#, costs = error_cost)
ins2_pred = predict(tree2_mod, tree_test)

plot(tree2_mod)

confusionMatrix(as.factor(ins2_pred), as.factor(tree_test$true), positive = "1")

cm = table(as.factor(ins2_pred), as.factor(tree_test$true))
cm
#final_profit = calcu(cm)
#final_profit

```


## Majority Vote
```{r}
#final predictions
final_pred = ifelse(logpreds + knnpreds + annpreds >= 2, 1,0)
cm = table(as.factor(final_pred), as.factor(ins_test_labels))
cm
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
final_profit = calcu(fp,tp, old_rate)
final_profit
```
By combining the three predictions from logistical regression, K-nearest neighbors, and ANN, we can find the majority classification, and find the final expected profit as $4335.

