---
title: "project2"
author: "Josh Richman"
date: "2023-11-02"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(neuralnet)
library(kernlab)
library(C50)
library(class)
```


# Preparing Data
## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
ins <- read.csv("TravelInsurancePrediction.csv", stringsAsFactors = TRUE)
#summary(ins)
ins$X = NULL

str(ins)
```

## Data Description & Summary Statistics
- **Age** - *Age of the customer*
  - Ranges from 25 to 35 years, suggesting that the dataset is focused on young adults.
- **Employment.Type** - *The Sector In Which Customer Is Employed*
  - Two types of employment: ‘Government Sector’ and ‘Private Sector/Self Employed’.The majority of individuals (1417) are employed in the ‘Private Sector/Self Employed’, while 570 are in the ‘Government Sector’.
- **GraduateOrNot** - *Whether The Customer Is College Graduate Or Not*
  - The majority of the individuals (1692) are graduates, compared to 295 who are not.
- **AnnualIncome** - *Yearly Income Of The Customer*
  - Ranges from 300,000 to 1,800,000. The mean annual income is 932,763. The income values spread over a wide range from the minimum to maximum, suggesting a diverse customer base in terms of economic status.
- **FamilyMembers** - *Number Of Family Members Living With Customer*
  - Ranges from 2 to 9 members. The average family has approximately 4.75 members which is significantly higher than the average size of a family in the US
- **ChronicDiseases** - *Whether Customer Has Any Chronic Conditions*
  - Binary variable. The majority of individuals (1570) are not frequent flyers, with only 417 being frequent flyers.
- **FrequentFlyer** - *Whether A Customer Books Frequent Air Tickets*
  - The majority of individuals (1570) are not frequent flyers, with only 417 being frequent flyers.
- **EverTravelledAbroad** - *Has The Customer Ever Travelled To A Foreign Country*
  - Binary variable. 1607 individuals have not travelled abroad and 380 have.
- **TravelInsurance** - *Whether The Customer Bought The Travel Insurance Or Not*   
  - The binary target variable. 35.73% of the individuals have travel insurance.


## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

insmm <- as.data.frame(model.matrix(~.-1,ins))
str(insmm)

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything for KNN and ANN
ins_norm <- as.data.frame(lapply(insmm, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(ins_norm), 0.2*nrow(ins_norm)) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
ins_train <- ins_norm[-test_set, -match("TravelInsurance",names(ins_norm))]
ins_test <- ins_norm[test_set, -match("TravelInsurance",names(ins_norm))]

#Now the response (aka Labels) - only the yyes column
ins_train_labels <- ins_norm[-test_set, "TravelInsurance"]
ins_test_labels <- ins_norm[test_set, "TravelInsurance"]

#train and test set that includes yyes
ins_train_all = ins_norm[-test_set,]
ins_test_all = ins_norm[test_set,]
```

# Majority Vote Algorithms
## Functions
```{r}
#fp = false positives (customers we called but did not buy a term deposit).
#tp = true positives (customers we called and they did buy a term deposit).
#old_rate = old call success rate, based on test data from confusion matrix, usually around 0.11.
#calcu = function(fp,tp, old_rate) {
  #revenue
#  rev = tp * 11
  
  #cost
#  total_calls = tp + fp
#  new_rate = tp / total_calls
#  avg_employee_calls = ((new_rate - old_rate) * 100 * 100) + 1000
#  num_employees = ceiling(total_calls / avg_employee_calls)
#  train_cost = num_employees * 1000
#  cost = (total_calls * 1) + train_cost
#  return(rev - cost)
#}

#Finds best value to split data
#find_max = function(pred){
  #Sequence of values for weighing predictions
#  vals = seq(0.005,0.5,by=0.005)
#  max = 0
#  max_x = 0.005
#  for (x in vals){
#  preds = ifelse(pred > x, 1,0)
#  cm = table(as.factor(preds), as.factor(ins_test_labels))
#  fp = cm[2,1]
#  tp = cm[2,2]
#  old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
#  hold = calcu(fp,tp, old_rate)
#    if(hold > max){ 
#      max = hold
#      max_x = x
#    }
#  }
#  return(c(max_x,max))
#}
```

## Logistical Regression
```{r}
library(caret)
logmod = glm(TravelInsurance ~  . + (. * .), data=ins_train_all, family = 'binomial')
#summary(logmod)
#stepfin = step(logmod, direction = "backward")

logpred = predict(logmod, ins_test_all)

#maxs = find_max(logpred)

logpreds = ifelse(logpred > 0.45, 1,0)
cm = table(as.factor(logpreds), as.factor(ins_test_labels))
cm

cm2 = confusionMatrix(as.factor(logpreds), as.factor(ins_test_labels),positive = "1")
cm2
```

## K-Nearest Neighbors
```{r}
library(caret)
set.seed(12345)

#may need to change k still
knnpreds = knn(ins_train, ins_test, ins_train_labels, k=7, prob=TRUE)
#knnmod = knn(ins_train_labels ~ ., data=ins_train, k=7)
# knnpred = predict(knnmod, ins_test)
#best appears to be k=30 at 0.125
#maxs = find_max(knnpred)
#maxs
#knnpreds = ifelse(knnpred > 0.45 ,1,0)
cm = table(as.factor(knnpreds), as.factor(ins_test_labels))
#cm = confusionMatrix(as.factor(knnpreds),as.factor(tele_test_labels), positive = "1")
cm
#fp = cm[2,1]
#tp = cm[2,2]
#old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
#knnprof = calcu(fp,tp, old_rate)
#knnprof

```

## ANN
```{r}
# simple ANN with only a single hidden neuron
ANN_model <- neuralnet(formula = TravelInsurance ~ ., data = ins_train_all)


# visualize the network topology
plot(ANN_model)

## Step 4: Evaluating model performance ----
# obtain model results
model_results <- compute(ANN_model, ins_test_all[1:10])
# obtain predicted strength values
ann_predicted_yes <- model_results$net.result
annpreds = ifelse(ann_predicted_yes > 0.45 ,1,0)
cm = table(as.factor(annpreds), as.factor(ins_test_labels))
cm
#fp = cm[2,1]
#tp = cm[2,2]
#old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
#annprof = calcu(fp,tp, old_rate)
#annprof
```

## SVM
```{r}
svm_mod = ksvm(TravelInsurance ~ . ,data = ins_train_all,kernel = 'rbfdot')
svmpred = predict(svm_mod, ins_test_all)

#maxs = find_max(svmpred)
#maxs
svmpreds = ifelse(svmpred > 0.45 ,1,0)
cm = table(as.factor(svmpreds), as.factor(ins_test_labels))
cm
#svmprof = calcu(cm)
#svmprof
```

## Tree
```{r}
treemod = C5.0(as.factor(TravelInsurance) ~ ., data=ins_train_all)
treepred = predict(treemod, ins_test_all)
#confusionMatrix(as.factor(hrpred), as.factor(hr_test$left), positive="1")
cm = table(as.factor(treepred), as.factor(ins_test_labels))
cm
#treeprof = calcu(cm)
#treeprof
```

## Stacked Model
```{r}
#final predictions
ins_preds = data.frame(
  log = logpreds,
  knn = knnpreds,
  ann = annpreds,
  svm = svmpreds,
  tree = treepred,
  true = ins_test_labels
)
```

```{r}
#final second model can be better than all
#try to get higher kappa
set.seed(12345)

tree_train_idx = sample(1:nrow(ins_preds), 0.6*nrow(ins_preds))

tree_test = ins_preds[-tree_train_idx,]
tree_train = ins_preds[tree_train_idx,]

# find better cost
#error_cost = matrix(c(0,1,10,0), nrow=2)
#error_cost
tree2_mod = C5.0(as.factor(true) ~ ., data = ins_preds)#, costs = error_cost)
ins2_pred = predict(tree2_mod, tree_test)

plot(tree2_mod)

confusionMatrix(as.factor(ins2_pred), as.factor(tree_test$true), positive = "1")

cm = table(as.factor(ins2_pred), as.factor(tree_test$true))
cm
#final_profit = calcu(cm)
#final_profit

```


## Majority Vote (Other)
```{r}
#final predictions
final_pred = ifelse(logpreds + knnpreds + annpreds + svmpreds + treepred >= 3, 1,0)
cm = table(as.factor(final_pred), as.factor(ins_test_labels))
cm
#fp = cm[2,1]
#tp = cm[2,2]
#old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
#final_profit = calcu(fp,tp, old_rate)
#final_profit
```
