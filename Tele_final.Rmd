---
title: "Group Project: Telemarketing"
author: "Add Names Here"
date: "10/12/2023"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(neuralnet)
```


# Preparing Data
## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything for KNN and ANN
tele_norm <- as.data.frame(lapply(telemm, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]

#train and test set that includes yyes
tele_train_all = tele_norm[-test_set,]
tele_test_all = tele_norm[test_set,]
```

# Clusters
## Training the model

```{r}
set.seed(12345)

# Removing the Y column
tele_data <- telemm[ , -53]

# z-Scaling the data
tele_z <- as.data.frame(lapply(tele_data, scale))

tele_clusters <- kmeans(tele_z, 6)

tele_clusters$centers


```
## Visualizations to support cluster descriptions

The visualization below shows the distribution of ages in all the clusters. This gives us more insights into the age values, which aren't very useful as the values don't deviate a lot from average age in all clusters which is ~ 40 years. We can see cluster 2 and 5 are the only clusters including people over the age of 60 which aligns with the insight from the 'jobretired' that suggests most of the retired people are in these groups.

```{r}
# Assuming you have these libraries loaded
library(ggplot2)

# Adding cluster assignments to your original data
tele_data$cluster <- tele_clusters$cluster

# Creating the histogram using ggplot2
ggplot(tele_data, aes(x=age, fill=factor(cluster))) + 
  geom_histogram(binwidth = 10, position="identity", alpha=1) +
  xlim(0, 100) +
  labs(title = "Age Distribution for Each Cluster",
       x = "Age",
       y = "Count",
       fill = "Cluster") +
       facet_wrap(~ cluster, ncol = 3) + # Separate histograms for each cluster

  theme_minimal()

```

The line graphs below shows the average succesrate for all ages. Here it is evident that the young and older people tend to say 'Yes' more often. However, the significant fluctuations are caused by the fewer but succesful calls to the ages.

```{r}
library(dplyr)

avg_yyes_by_age <- telemm %>%
  group_by(age) %>%
  summarise(average_yyes = mean(yyes, na.rm = TRUE))

library(ggplot2)

ggplot(avg_yyes_by_age, aes(x = age, y = average_yyes)) +
  geom_point() + 
  geom_line(group = 1, color = "blue") + 
  labs(title = "Average yyes by Age", x = "Age", y = "Average yyes") +
  theme_minimal()
```


```{r}

telemm$cluster <- tele_clusters$cluster

# Step 1: Store the results of the aggregate function
agg_data <- aggregate( data = telemm, yyes ~ cluster, mean)

# Step 2: Create a new column for the size
agg_data$cluster_size <- NA  # Initialize with NA

# Step 3: Assign the sizes to the appropriate clusters
for(i in 1:nrow(agg_data)) {
  agg_data$cluster_size[i] <- tele_clusters$size[agg_data$cluster[i]]
}

# Step 4: Calculate profit for all clusters

# OLD PROFIT FUNCTION 
# agg_data$profit <- ((11 * agg_data$cluster_size) * agg_data$yyes) - (2 * agg_data$cluster_size)

calls_made = agg_data$cluster_size
succesful_calls = agg_data$cluster_size*agg_data$yyes
success_rate <- agg_data$yyes


old_success_rate <- mean(tele_norm$y)


calls_per_agent <- ((success_rate - old_success_rate) * 100 * 100) + 1000


costs <- calls_made * (1000 / calls_per_agent) + (calls_made * 1)
revenue <- succesful_calls * 11
  
agg_data$profit2 <- round(revenue - costs, 2)

# View the result
agg_data

```
## Description of profitable clusters

The two most profitable clusters are 2 and 5, predominantly comprising students and retirees. Given the product in focus - term deposits â€” it's understandable why these groups are interested. With their limited income, the safety and short-term nature of term deposits appeal to them, making it an attractive option for both securing and growing their savings.

Cluster 2 primarily consists of individuals who've shown a positive response in past campaigns, as evidenced by high values in 'previous' and 'poutcomesuccess'. In contrast, Cluster 5 seems to be made up of those the call center hasn't contacted before, given its high 'poutcomenonexistent' value and negative scores in the other two metrics.

Cluster 2: Returning Positive Buyers
Cluster 5: Untouched Potential


# Majority Vote Algorithms
## Functions
```{r}
#fp = false positives (customers we called but did not buy a term deposit).
#tp = true positives (customers we called and they did buy a term deposit).
#old_rate = old call success rate, based on test data from confusion matrix, usually around 0.11.
calcu = function(fp,tp, old_rate) {
  #revenue
  rev = tp * 11
  
  #cost
  total_calls = tp + fp
  new_rate = tp / total_calls
  avg_employee_calls = ((new_rate - old_rate) * 100 * 100) + 1000
  num_employees = ceiling(total_calls / avg_employee_calls)
  train_cost = num_employees * 1000
  cost = (total_calls * 1) + train_cost
  return(rev - cost)
}

#Finds best value to split data
find_max = function(pred){
  #Sequence of values for weighing predictions
  vals = seq(0.005,0.5,by=0.005)
  max = 0
  max_x = 0.005
  for (x in vals){
  preds = ifelse(pred > x, 1,0)
  cm = table(as.factor(preds), as.factor(tele_test_labels))
  fp = cm[2,1]
  tp = cm[2,2]
  old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
  hold = calcu(fp,tp, old_rate)
    if(hold > max){ 
      max = hold
      max_x = x
    }
  }
  return(c(max_x,max))
}
```

## logistical Regression
```{r}
#set.seed(12345)
#(educationilliterate + day_of_weektue + monthjul + jobadmin. + educationuniversity.degree + jobtechnician + day_of_weekthu + day_of_weekmon + monthdec + jobstudent + monthnov + day_of_weekwed + monthoct + monthsep + jobretired + campaign + I(cons.price.idx**2) + monthmay + defaultunknown + poutcomesuccess + pdaysdummy + monthaug + monthjun + nr.employed + cons.conf.idx + poutcomenonexistent + contacttelephone + cons.price.idx + emp.var.rate + monthmar)
library(caret)
logmod = glm(yyes ~  I(emp.var.rate**2) + (emp.var.rate * .) + . + (defaultunknown * contacttelephone) + (campaign * contacttelephone) + (cons.price.idx * .) + I(cons.price.idx**2), data=tele_train_all, family = 'binomial')
#summary(logmod)
#stepfin = step(logmod, direction = "backward")

logpred = predict(logmod, tele_test_all)
#logpred

maxs = find_max(logpred)
#maxs
logpreds = ifelse(logpred > maxs[1], 1,0)
cm = table(as.factor(logpreds), as.factor(tele_test_labels))
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
profit = calcu(fp,tp, old_rate)
profit
```

## K-Nearest Neighbors
```{r}
library(caret)
set.seed(12345)

#may need to change k still
knnmod = knnreg(tele_train_labels ~ ., data=tele_train, k=30)
knnpred = predict(knnmod, tele_test)
#best appears to be k=30 at 0.125
maxs = find_max(knnpred)
maxs
knnpreds = ifelse(knnpred > maxs[1] ,1,0)
cm = table(as.factor(knnpreds), as.factor(tele_test_labels))
#cm = confusionMatrix(as.factor(knnpreds),as.factor(tele_test_labels), positive = "1")
cm
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
knnprof = calcu(fp,tp, old_rate)
knnprof

```

## ANN
```{r}
# simple ANN with only a single hidden neuron
teleANN_model <- neuralnet(formula = yyes ~ ., data = tele_train_all)


# visualize the network topology
plot(teleANN_model)

## Step 4: Evaluating model performance ----
# obtain model results
model_results <- compute(teleANN_model, tele_test_all[1:54])
# obtain predicted strength values
ann_predicted_yes <- model_results$net.result
annpreds = ifelse(ann_predicted_yes > 0.125 ,1,0)
cm = table(as.factor(annpreds), as.factor(tele_test_labels))
cm
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
annprof = calcu(fp,tp, old_rate)
annprof
```

## Majority Vote
```{r}
#final predictions
final_pred = ifelse(logpreds + knnpreds + annpreds >= 2, 1,0)
cm = table(as.factor(final_pred), as.factor(tele_test_labels))
cm
fp = cm[2,1]
tp = cm[2,2]
old_rate = (tp + cm[1,2]) / (fp + cm[1,1] + tp + cm[1,2])
final_profit = calcu(fp,tp, old_rate)
final_profit
```
By combining the three predictions from logistical regression, K-nearest neighbors, and ANN, we can find the majority classification, and find the final expected profit as $4335.

